{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2014fb-6c7c-4aa0-9f7b-c52cc1abe862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de44f128-4a63-455f-9f9f-3e08c66d884a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd8a1c-2275-4057-89ce-fd2340ef109e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fetch the dataset from the SageMaker bucket\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.download_file(\n",
    "    f\"sagemaker-sample-files\", \"datasets/tabular/uci_abalone/abalone.csv\", \"./data/abalone.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec91e7-e5a9-4a34-8f0a-066bca6e137c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./code/preprocess.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    "    VectorIndexer,\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType\n",
    ")\n",
    "\n",
    "def csv_line(data):\n",
    "    r = \",\".join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_input_bucket\", type=str, help = \"s3 inupt bucket\")\n",
    "    parser.add_argument(\"--s3_input_key_prefix\", type=str, help=\"s3 input key prefix\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "    \n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "    \n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"sex\", StringType(), True),\n",
    "            StructField(\"length\", DoubleType(), True),\n",
    "            StructField(\"diameter\", DoubleType(), True),\n",
    "            StructField(\"height\", DoubleType(), True),\n",
    "            StructField(\"whole_weight\", DoubleType(), True),\n",
    "            StructField(\"shucked_weight\", DoubleType(), True),\n",
    "            StructField(\"viscera_weight\", DoubleType(), True),\n",
    "            StructField(\"shell_weight\", DoubleType(), True),\n",
    "            StructField(\"rings\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(\n",
    "        (\"s3://\" + os.path.join(args.s3_input_bucket, args.s3_input_key_prefix, \"abalone.csv\")),\n",
    "        header=False,\n",
    "        schema=schema,\n",
    "    )\n",
    "    \n",
    "    # StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\")\n",
    "    \n",
    "    # one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "    \n",
    "    # vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\n",
    "            \"sex_vec\",\n",
    "            \"length\",\n",
    "            \"diameter\",\n",
    "            \"height\",\n",
    "            \"whole_weight\",\n",
    "            \"shucked_weight\",\n",
    "            \"viscera_weight\",\n",
    "            \"shell_weight\",\n",
    "        ],\n",
    "        outputCol=\"features\",\n",
    "    )\n",
    "    \n",
    "    # The pipeline is comprised of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "\n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "\n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "\n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile(\n",
    "        \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"train\")\n",
    "    )\n",
    "\n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile(\n",
    "        \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"validation\")\n",
    "    )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8426de2-f948-4f42-822a-ac9f9ff173bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Upload the raw input dataset to a unique S3 location\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_abalone = \"{}/input/raw/abalone\".format(prefix)\n",
    "input_preprocessed_prefix_abalone = \"{}/input/preprocessed/abalone\".format(prefix)\n",
    "\n",
    "sagemaker_session.upload_data(\n",
    "    path=\"./data/abalone.csv\", bucket=bucket, key_prefix=input_prefix_abalone\n",
    ")\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"3.1\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/preprocess.py\",\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_input_key_prefix\",\n",
    "        input_prefix_abalone,\n",
    "        \"--s3_output_bucket\",\n",
    "        bucket,\n",
    "        \"--s3_output_key_prefix\",\n",
    "        input_preprocessed_prefix_abalone,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, prefix),\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8ef5e8-976e-4feb-8336-ac15d2e46adc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, input_preprocessed_prefix_abalone))\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix_abalone/train/part-00000 - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3485834-49a8-4c0a-bd86-2d65d8b936de",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor.start_history_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c23df4-8d7e-4b3b-b9fd-a9c2ea1a0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor.terminate_history_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a35e0-236c-4928-81af-bff5633e4930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./code/hello_py_spark_app.py\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "# Import local module to test spark-submit--py-files dependencies\n",
    "import hello_py_spark_udfs as udfs\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Hello World, this is PySpark!\")\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"inputs and outputs\")\n",
    "    parser.add_argument(\"--input\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--output\", required=False, type=str, help=\"path to output data\")\n",
    "    args = parser.parse_args()\n",
    "    spark = SparkSession.builder.appName(\"SparkTestApp\").getOrCreate()\n",
    "    sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "    # Load test data set\n",
    "    inputPath = args.input\n",
    "    outputPath = args.output\n",
    "    salesDF = spark.read.json(inputPath)\n",
    "    salesDF.printSchema()\n",
    "\n",
    "    salesDF.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "    # Define a UDF that doubles an integer column\n",
    "    # The UDF function is imported from local module to test spark-submit--py-files dependencies\n",
    "    double_udf_int = udf(udfs.double_x, IntegerType())\n",
    "\n",
    "    # Save transformed data set to disk\n",
    "    salesDF.select(\"date\", \"sale\", double_udf_int(\"sale\").alias(\"sale_double\")).write.json(\n",
    "        outputPath\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9188b99-15d4-42a3-bc91-be5636653b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./code/hello_py_spark_udfs.py\n",
    "def double_x(x):\n",
    "    return x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef22cf-f45d-40b4-a1ea-3aaf1d9fd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define job input/output URIs\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_sales = \"{}/input/sales\".format(prefix)\n",
    "output_prefix_sales = \"{}/output/sales\".format(prefix)\n",
    "input_s3_uri = \"s3://{}/{}\".format(bucket, input_prefix_sales)\n",
    "output_s3_uri = \"s3://{}/{}\".format(bucket, output_prefix_sales)\n",
    "\n",
    "sagemaker_session.upload_data(\n",
    "    path=\"./data/data.jsonl\", bucket=bucket, key_prefix=input_prefix_sales\n",
    ")\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark-udfs\",\n",
    "    framework_version=\"3.1\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/hello_py_spark_app.py\",\n",
    "    submit_py_files=[\"./code/hello_py_spark_udfs.py\"],\n",
    "    arguments=[\"--input\", input_s3_uri, \"--output\", output_s3_uri],\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31d049-88cc-4950-a6a9-f3e2ec0f22df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
